<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/print.css" media="print">

    <title>Awesome Self-Driving Cars by takeitallsource</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>Awesome Self-Driving Cars</h1>
        <h2>Curated List of Self-Driving Cars Resources</h2>

        <section id="downloads">
          <a href="https://github.com/takeitallsource/awesome-autonomous-vehicles/zipball/master" class="btn">Download as .zip</a>
          <a href="https://github.com/takeitallsource/awesome-autonomous-vehicles/tarball/master" class="btn">Download as .tar.gz</a>
          <a href="https://github.com/takeitallsource/awesome-autonomous-vehicles" class="btn btn-github"><span class="icon"></span>View on GitHub</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h1>
<a id="awesome-autonomous-vehicles-" class="anchor" href="#awesome-autonomous-vehicles-" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Awesome Autonomous Vehicles: <a href="https://github.com/sindresorhus/awesome"><img src="https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg" alt="Awesome"></a>
</h1>

<p>A curated list of awesome autonomous vehicles resources, inspired by <a href="https://github.com/ziadoz/awesome-php">awesome-php</a>.</p>

<h2>
<a id="contributing" class="anchor" href="#contributing" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Contributing</h2>

<p>Please feel free to send me pull requests or email me at (<a href="mailto:takeitallsource@gmail.com">takeitallsource@gmail.com</a>) to add links.</p>

<h2>
<a id="table-of-contents" class="anchor" href="#table-of-contents" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Table of Contents</h2>

<ul>
<li>Foundations

<ul>
<li>Artificial Intelligence</li>
<li>Computer Vision</li>
<li>Robotics</li>
<li>Human-Machine Interaction</li>
</ul>
</li>
<li>Courses</li>
<li>Papers</li>
<li>Research Labs</li>
<li>Datasets</li>
<li>Open Source Software</li>
<li>Companies</li>
<li>Legal Frame</li>
<li>Blogs</li>
<li>Social Media</li>
<li>Miscellaneous</li>
</ul>

<h2>
<a id="foundations" class="anchor" href="#foundations" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Foundations</h2>

<h3>
<a id="artificial-intelligence" class="anchor" href="#artificial-intelligence" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Artificial Intelligence</h3>

<ol>
<li>
<a href="https://github.com/josephmisiti/awesome-machine-learning">Awesome Machine Learning</a> - A curated list of awesome Machine Learning frameworks, libraries and software. Maintained by Joseph Misiti.Joseph Misiti</li>
<li>
<a href="https://github.com/songrotek/Deep-Learning-Papers-Reading-Roadmap">Deep Learning Papers Reading Roadmap</a> - Deep Learning papers reading roadmap constructed from outline to detail, old to state-of-the-art,
from generic to specific areas focus on state-of-the-art for anyone starting in Deep Learning. Maintained by, Flood Sung.</li>
<li>
<a href="http://www.deeplearningweekly.com/pages/open_source_deep_learning_curriculum">Open Source Deep Learning Curriculum</a> - Deep Learning curriculum  meant to be a starting point for everyone interested in seriously studying the field.</li>
</ol>

<h3>
<a id="robotics" class="anchor" href="#robotics" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Robotics</h3>

<ol>
<li>
<a href="https://github.com/Kiloreux/awesome-robotics">Awesome Robotics</a> - A list of various books, courses and other resources for robotics, maintained by kiloreux.</li>
</ol>

<h3>
<a id="computer-vision" class="anchor" href="#computer-vision" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Computer Vision</h3>

<ol>
<li>
<a href="https://github.com/jbhuang0604/awesome-computer-vision">Awesome Computer Vision</a> - A curated list of awesome computer vision resources, maintained by Jia-Bin Huang</li>
<li>
<a href="https://github.com/kjw0612/awesome-deep-vision">Awesome Deep Vision</a> - A curated list of deep learning resources for computer vision, maintained by Jiwon Kim, Heesoo Myeong, Myungsub Choi, Jung Kwon Lee, Taeksoo Kim</li>
</ol>

<h3>
<a id="human-machine-interaction" class="anchor" href="#human-machine-interaction" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Human-Machine Interaction</h3>

<p>1.</p>

<h2>
<a id="courses" class="anchor" href="#courses" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Courses</h2>

<ol>
<li>
<a href="https://www.udacity.com/course/self-driving-car-engineer-nanodegree--nd013">[Udacity] Self-Driving Car Nanodegree Program</a> - teaches the skills and techniques used by self-driving car teams. Program syllabus can be found <a href="https://medium.com/self-driving-cars/term-1-in-depth-on-udacitys-self-driving-car-curriculum-ffcf46af0c08#.bfgw9uxd9">here</a>.</li>
<li>
<a href="http://www.cs.toronto.edu/%7Eurtasun/courses/CSC2541/CSC2541_Winter16.html">[University of Toronto] CSC2541
Visual Perception for Autonomous Driving</a> - A graduate course in visual perception for autonomous driving. The class briefly covers topics in localization, ego-motion estimaton, free-space estimation, visual recognition (classification, detection, segmentation).</li>
<li>
<a href="https://www.fun-mooc.fr/courses/inria/41005S02/session02/about?utm_source=mooc-list">[INRIA] Mobile Robots and Autonomous Vehicles</a> - Introduces the key concepts required to program mobile robots and autonomous vehicles. The course presents both formal and algorithmic tools, and for its last week's topics (behavior modeling and learning), it will also provide realistic examples and programming exercises in Python.</li>
<li>
<a href="http://www.gla.ac.uk/coursecatalogue/course/?code=ENG5017">[Universty of Glasgow] ENG5017 Autonomous Vehicle Guidance Systems</a> - Introduces the concepts behind autonomous vehicle guidance and coordination and enables students to design and implement guidance strategies for vehicles incorporating planning, optimising and reacting elements.</li>
<li>
<a href="http://duckietown.mit.edu/index.html">[MIT] 2.166 Duckietown</a> - Class about the science of autonomy at the graduate level. This is a hands-on, project-focused course focusing on self-driving vehicles and high-level autonomy. The problem: <strong>Design the Autonomous Robo-Taxis System for the City of Duckietown.</strong>
</li>
<li>
<a href="https://medium.com/self-driving-cars/how-to-land-an-autonomous-vehicle-job-coursework-e7acc2bfe740#.j5b2kwbso">[David Silver - Udacity] How to Land An Autonomous Vehicle Job: Coursework</a> David Silver, from Udacity, reviews his coursework for landing a job in self-driving cars coming from a Software Engineering background.</li>
<li>
<a href="http://stanford.edu/%7Ecpiech/cs221/index.html">[Stanford] - CS221 Artificial Intelligence: Principles and Techniques</a> - Contains a simple self-driving project and simulator.</li>
</ol>

<h2>
<a id="papers" class="anchor" href="#papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Papers</h2>

<p>By Topic Areas and Year of Publication / Submission</p>

<h4>
<a id="general" class="anchor" href="#general" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>General</h4>

<ol>
<li>
<strong>[2016]</strong> <em>Combining Deep Reinforcement Learning and Safety Based Control for Autonomous Driving</em>. [<a href="https://arxiv.org/abs/1612.00147">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>End to End Learning for Self-Driving Cars</em>. [<a href="https://arxiv.org/abs/1604.07316">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>An Empirical Evaluation of Deep Learning on Highway Driving</em>. [<a href="https://arxiv.org/abs/1504.01716">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Self-Driving Vehicles: The Challenges and Opportunities Ahead</em>. [<a href="http://dl.acm.org/citation.cfm?id=2823464">ref</a>]</li>
<li>
<strong>[2013]</strong> <em>Towards a viable autonomous driving research platform</em>. [<a href="https://www.semanticscholar.org/paper/Towards-a-viable-autonomous-driving-research-Wei-Snider/da5cee7a6eb817bbbf4721c64c756bd8b7122359">ref</a>]</li>
<li>
<strong>[2010]</strong> <em>Toward robotic cars</em>. [<a href="http://dl.acm.org/citation.cfm?id=1721679">ref</a>]</li>
<li>
<strong>[2008]</strong> <em>Autonomous Driving in Urban Environments: Boss and the Urban Challenge</em>. [<a href="https://www.semanticscholar.org/paper/Autonomous-Driving-in-Urban-Environments-Boss-and-Urmson-Anhalt/1c0fb6b1bbfde0f9bab6268f5609cce2bd3bc5bd">ref</a>]</li>
<li>
<strong>[2008]</strong> <em>Caroline: An autonomously driving vehicle for urban environments</em>. [<a href="https://www.semanticscholar.org/paper/Caroline-An-autonomously-driving-vehicle-for-urban-Rauskolb-Berger/08f4e164291942fc78bd6945215b2c672b17edd5">ref</a>]</li>
<li>
<strong>[2007]</strong> <em>Self-Driving Cars - An AI-Robotics Challenge</em>. [<a href="https://www.semanticscholar.org/paper/Self-Driving-Cars-An-AI-Robotics-Challenge-Thrun/31d17c77d2ea18f71d570741665f0fd3030caa94">ref</a>]</li>
<li>
<strong>[2006]</strong> <em>A Personal Account of the Development of Stanley, the Robot That Won the DARPA Grand Challenge</em>. [<a href="https://www.semanticscholar.org/paper/A-Personal-Account-of-the-Development-of-Stanley-Thrun/74a4de58be068d2dc38bb31cf54c3c49bdc0d4e4">ref</a>]</li>
<li>
<strong>[2006]</strong> <em>Stanley: The robot that won the DARPA Grand Challenge</em>. [<a href="https://www.semanticscholar.org/paper/Stanley-The-robot-that-won-the-DARPA-Grand-Thrun-Montemerlo/298500897243b17fa2ebe7bde0a1b8ebc00ea07f">ref</a>]</li>
</ol>

<h4>
<a id="localization--mapping" class="anchor" href="#localization--mapping" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Localization &amp; Mapping</h4>

<ol>
<li>
<strong>[2016]</strong> <em>MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System.</em> [<a href="https://arxiv.org/abs/1610.07336">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Image Based Camera Localization: an Overview</em>. [<a href="https://arxiv.org/abs/1610.03660">ref</a>]</li>
<li>
<strong>[2007]</strong> <em>Map-Based Precision Vehicle Localization in Urban Environments</em>. [<a href="https://www.semanticscholar.org/paper/Map-Based-Precision-Vehicle-Localization-in-Urban-Levinson-Montemerlo/924f7268d592d327f97ad4e96f48ad774d982ef3">ref</a>]</li>
</ol>

<h4>
<a id="perception" class="anchor" href="#perception" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Perception</h4>

<ol>
<li>
<strong>[2016]</strong> <em>VisualBackProp: visualizing CNNs for autonomous driving</em>. [<a href="https://arxiv.org/abs/1611.05418">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Driving in the Matrix: Can Virtual Worlds Replace Human-Generated Annotations for Real World Tasks?</em>. [<a href="https://arxiv.org/abs/1610.01983">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Lost and Found: Detecting Small Road Hazards for Self-Driving Vehicles</em>. [<a href="https://arxiv.org/abs/1609.04653">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Image segmentation of cross-country scenes captured in IR spectrum</em>. [<a href="https://arxiv.org/abs/1604.02469">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Traffic-Sign Detection and Classification in the Wild</em>. [<a href="https://www.semanticscholar.org/paper/Traffic-Sign-Detection-and-Classification-in-the-Zhu-Liang/d463499b7a82e3cad81d2471b52a198b857aa75b">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Pixel-wise Segmentation of Street with Neural Networks</em>. [<a href="https://arxiv.org/abs/1511.00513">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Deep convolutional neural networks for pedestrian detection</em>. [<a href="https://arxiv.org/abs/1510.03608">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Fast Algorithms for Convolutional Neural Networks</em>. [<a href="https://arxiv.org/abs/1509.09308">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Fusion of color images and LiDAR data for lane classification</em>. [<a href="http://dl.acm.org/citation.cfm?id=2820859">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Rover – a Lego* Self-driving Car</em>. [<a href="https://www.semanticscholar.org/paper/Rover-a-Lego-Self-driving-Car-Tan-Wojtczyk-Wojtczyk/6e24123ef558ffb9888d28f992f8afe76622830e">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Classification and Tracking of Dynamic Objects with Multiple Sensors for Autonomous Driving in Urban Environments</em>. [<a href="https://www.semanticscholar.org/paper/Classification-and-Tracking-of-Dynamic-Objects-Darms-Rybski/6c9ce40060fa3efea7d04a4a0e36609592ed6ddf">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Generating Omni-directional View of Neighboring Objects for Ensuring Safe Urban Driving</em>. [<a href="https://www.semanticscholar.org/paper/Generating-Omni-directional-View-of-Neighboring-Seo/29e53add392de54d439a6002c67e8af6e9baadeb">ref</a>]</li>
<li>
<strong>[2013]</strong> <em>Focused Trajectory Planning for autonomous on-road driving</em>. [<a href="https://www.semanticscholar.org/paper/Focused-Trajectory-Planning-for-autonomous-on-road-Gu-Snider/03bf26d72d8cc0cf401c31e31c242e1894bd0890">ref</a>]</li>
<li>
<strong>[2008]</strong> <em>Vehicle detection and tracking for the Urban Challenge</em>. [<a href="https://www.semanticscholar.org/paper/Vehicle-detection-and-tracking-for-the-Urban-Darms-Baker/757fbaa9881b9075409a9962819fda64d51307e1">ref</a>]</li>
<li>
<strong>[2000]</strong> <em>Real-time multiple vehicle detection and tracking from a moving vehicle</em>. [<a href="https://www.semanticscholar.org/paper/Real-time-multiple-vehicle-detection-and-tracking-Betke-Haritaoglu/864a7068c346ecbc4ef6c4da66e4c8bcc83fe560">ref</a>]</li>
</ol>

<h4>
<a id="navigation--planning" class="anchor" href="#navigation--planning" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Navigation &amp; Planning</h4>

<ol>
<li>
<strong>[2016]</strong> <em>A Self-Driving Robot Using Deep Convolutional Neural Networks on Neuromorphic Hardware</em>. [<a href="https://arxiv.org/abs/1611.01235">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>A Survey of Motion Planning and Control Techniques for Self-driving Urban Vehicles</em>. [<a href="https://arxiv.org/abs/1604.07446">ref</a>]</li>
<li>
<strong>[2016]</strong> <em>Routing Autonomous Vehicles in Congested Transportation Networks: Structural Properties and Coordination Algorithms</em>. [<a href="https://arxiv.org/abs/1603.00939">ref</a>]</li>
<li>
<strong>[2013]</strong> <em>Motion Estimation for Self-Driving Cars with a Generalized Camera</em>. [<a href="https://www.semanticscholar.org/paper/Motion-Estimation-for-Self-Driving-Cars-with-a-Lee-Fraundorfer/f7f775a4f484706ffbc524accb351cb564469f6a">ref</a>]</li>
<li>
<strong>[2015]</strong> <em>Model Predictive Control of Autonomous Mobility-on-Demand Systems</em>. [<a href="https://arxiv.org/abs/1509.03985">ref</a>]</li>
<li>
<strong>[2008]</strong> <em>Motion Planning in Urban Environments</em>. [<a href="https://www.semanticscholar.org/paper/Motion-Planning-in-Urban-Environments-Ferguson-Howard/8fa74131756a50c1562ebf1f03552779803aed67">ref</a>]</li>
<li>
<strong>[2007]</strong> <em>Online Speed Adaptation Using Supervised Learning for High-Speed, Off-Road Autonomous Driving</em>.[<a href="https://www.semanticscholar.org/paper/Online-Speed-Adaptation-Using-Supervised-Learning-Stavens-Hoffmann/9db82954df3f4ae829459dcb8719b8a8ed9f4bee">ref</a>]</li>
<li>
<strong>[2006]</strong> <em>Probabilistic Terrain Analysis For High-Speed Desert Driving</em>.[<a href="https://www.semanticscholar.org/paper/Probabilistic-Terrain-Analysis-For-High-Speed-Thrun-Montemerlo/b23a7882b35d0252e5f3011bff15c6dca46ef84e">ref</a>]</li>
</ol>

<h4>
<a id="simulation" class="anchor" href="#simulation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Simulation</h4>

<ol>
<li>
<strong>[2016]</strong> <em>Learning a Driving Simulator</em>. [<a href="https://arxiv.org/abs/1608.01230">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>From a Competition for Self-Driving Miniature Cars to a Standardized Experimental Platform: Concept, Models, Architecture, and Evaluation</em>. [<a href="https://arxiv.org/abs/1406.7768">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Technical evaluation of the Carolo-Cup 2014 - A competition for self-driving miniature cars</em>. [<a href="https://www.semanticscholar.org/paper/Technical-evaluation-of-the-Carolo-Cup-2014-A-Zug-Steup/4f57643b95e854bb05fa0c037cbf8898accdbdef">ref</a>]</li>
</ol>

<h4>
<a id="software-engineering" class="anchor" href="#software-engineering" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Software Engineering</h4>

<ol>
<li>
<strong>[2016]</strong> <em>Evaluation of Sandboxed Software Deployment for Real-time Software on the Example of a Self-Driving Heavy Vehicle</em>. [<a href="https://arxiv.org/abs/1608.06759">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Engineering the Hardware/Software Interface for Robotic Platforms - A Comparison of Applied Model Checking with Prolog and Alloy</em>. [<a href="https://arxiv.org/abs/1401.3985">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Comparison of Architectural Design Decisions for Resource-Constrained Self-Driving Cars - A Multiple Case-Study</em>. [<a href="https://www.semanticscholar.org/paper/Comparison-of-Architectural-Design-Decisions-for-Berger-Dukaczewski/c89f47c93c62c107e6bd75acde89ee7417ebf244">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>(Re)liability of Self-driving Cars. An Interesting Challenge!</em>. [<a href="http://onlinelibrary.wiley.com/doi/10.1002/qre.1707/full">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Explicating, Understanding, and Managing Technical Debt from Self-Driving Miniature Car Projects</em>. [<a href="http://ieeexplore.ieee.org/document/6974884/">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Towards Continuous Integration for Cyber-Physical Systems on the Example of Self-Driving Miniature Cars</em>. [<a href="https://www.semanticscholar.org/paper/Towards-Continuous-Integration-for-Cyber-Physical-Berger/2ac2aa0285984f2ce57efa77aab4e372bbc3ee6c">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Saving virtual testing time for CPS by analyzing code coverage on the example of a lane-following algorithm</em>. [<a href="http://dl.acm.org/citation.cfm?id=2593466">ref</a>]</li>
<li>
<strong>[2013]</strong> <em>Parallel scheduling for cyber-physical systems: analysis and case study on a self-driving car</em>[<a href="http://dl.acm.org/citation.cfm?id=2502530">ref</a>]</li>
<li>
<strong>[2012]</strong> <em>SAFER: System-level Architecture for Failure Evasion in Real-time Applications</em>. [<a href="https://www.semanticscholar.org/paper/SAFER-System-level-Architecture-for-Failure-Kim-Bhatia/ff05797dcc041d04f9ed277269916ad6ff92f1f0">ref</a>]</li>
<li>
<strong>[2010]</strong> <em>Automating acceptance tests for sensor- and actuator-based systems on the example of autonomous vehicles</em>. [<a href="https://www.semanticscholar.org/paper/Automating-acceptance-tests-for-sensor-and-Berger/3bc567143118f8fb34e0460cc3424701683c2511">ref</a>]</li>
<li>
<strong>[2007]</strong> <em>Software &amp; Systems Engineering Process and Tools for the Development of Autonomous Driving Intelligence</em> [<a href="https://www.semanticscholar.org/paper/Software-Systems-Engineering-Process-and-Tools-for-Basarke-Berger/c564b62cd7df2ed47bb9a6266cc19c83024bc390">ref</a>]</li>
</ol>

<h4>
<a id="human-machine-interaction-1" class="anchor" href="#human-machine-interaction-1" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Human-Machine Interaction</h4>

<ol>
<li>
<strong>[2015]</strong> <em>User interface considerations to prevent self-driving carsickness</em>. [<a href="http://dl.acm.org/citation.cfm?id=2809754">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>A Survey of Public Opinion about Autonomous and Self-driving</em>.[<a href="https://www.semanticscholar.org/paper/A-Survey-of-Public-Opinion-about-Autonomous-and-Schoettle-Sivak/5d983c2d2160b9c159b2cdcfcfaded01a4ce2ad6">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Public Opinion about Self-driving Vehicles</em>. [<a href="https://www.semanticscholar.org/paper/Public-Opinion-about-Self-driving-Vehicles-Schoettle-Sivak/4984ed8ae3355d58cfad2bd27cb2bc2488cb0e6a">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Setting the Stage for Self-driving Cars: Exploration of Future Autonomous Driving Experiences</em>. [<a href="https://www.semanticscholar.org/paper/Setting-the-Stage-for-Self-driving-Cars-Pettersson/df428d8015b92902416d07379fb3415a12d64e3f">ref</a>]</li>
</ol>

<h4>
<a id="infrastructure" class="anchor" href="#infrastructure" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Infrastructure</h4>

<ol>
<li>
<strong>[2014]</strong> <em>Control of Robotic Mobility-On-Demand Systems: a Queueing-Theoretical Perspective</em>. [<a href="https://arxiv.org/abs/1404.4391">ref</a>]</li>
<li>
<strong>[2014]</strong> <em>Priority-based Intersection Control Framework for Self-Driving Vehicles: Agent-based Model Development and Evaluation</em>. [<a href="https://www.researchgate.net/publication/271738793_Priority-based_Intersection_Control_Framework_for_Self-Driving_Vehicles_Agent-based_Model_Development_and_Evaluation">ref</a>]</li>
<li>
<strong>[2005]</strong> <em>Cooperative autonomous driving: intelligent vehicles sharing city roads</em>. [<a href="https://www.semanticscholar.org/paper/Cooperative-autonomous-driving-intelligent-Baber-Kolodko/a42f42fa95d8ee6498dff905ed4848437a8f0084">ref</a>]</li>
</ol>

<h4>
<a id="law--society" class="anchor" href="#law--society" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Law &amp; Society</h4>

<ol>
<li>
<strong>[2016]</strong> <em>Autonomous Vehicle Technology: A Guide for Policymakers</em>. [<a href="https://www.semanticscholar.org/paper/Autonomous-Vehicle-Technology-A-Guide-for-Anderson-Kalra/a0231f6ab2a9feaef92d5481149cdb2142aaeb02">ref</a>]</li>
<li>
<strong>[2014]</strong> <em><strong>WHITE PAPER</strong> Self-driving Vehicles: Current Status of Autonomous Vehicle Development and Minnesota Policy Implications Preliminary White Paper</em>. [<a href="https://www.semanticscholar.org/paper/Self-driving-Vehicles-Current-Status-of-Autonomous-Lari-Douma/581075c89f6a3945fa43d61aac1329d1e43f9fa3">ref</a>]</li>
</ol>

<h2>
<a id="research-labs" class="anchor" href="#research-labs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Research Labs</h2>

<ol>
<li> <a href="https://cars.stanford.edu/">Center for Automotive Research at Stanford</a> - Current areas of research focuses on human-centered mobility themes like
understanding how people will interact with increasingly automated vehicles, societal impacts of vehicle automation from policy to ethics to law, technology advances in sensing, decision-making and control.</li>
<li>
<a href="http://aicenter.stanford.edu/research/">SAIL-TOYOTA Center for AI Research at Stanford</a> - The theme of the center is <strong>Human-Centered Artificial Intelligence for Future Intelligent Vehicles and Beyond.</strong>
</li>
<li>
<a href="http://www.path.berkeley.edu/berkeley-deepdrive">Berkeley DeepDrive</a> - Investigates state-of-the-art technologies in computer vision and machine learning for automotive application.</li>
<li>
<a href="http://pave.princeton.edu/">Princeton Autonomous Vehicle Engineering</a> - undergraduate student-led research group at Princeton University dedicated to advancing and promoting the field of robotics through competitive challenges, self-guided research and community outreach.</li>
<li>
<a href="http://www.avl.umd.edu/">University of Maryland Autonomous Vehicle Laboratory</a> - conducts research and development in the area of biologically inspired design and robotics.</li>
<li>
<a href="http://wavelab.uwaterloo.ca/">University of Waterloo WAVE Laboratory</a> - Research areas includes Multirotor UAV, Autonomous driving and Multi-Camera Parallel Tracking and Mapping.</li>
<li>
<a href="http://mrg.robots.ox.ac.uk/">Oxford Robotics Institute – Autonomous Systems</a> - Researches all aspects of land based mobile autonomy.</li>
<li>
<a href="http://autonomos-labs.com/">Autonomous Lab - Freie Universität Berlin</a> - Computer Vision, Cognitive Navigation, Spatial Car Environment Capture.</li>
<li>
<a href="http://usa.honda-ri.com/Pages/Research%20Area/Detail.aspx?listId=2">Honda Research Institute - USA</a> - engaged in development and integration of multiple sensory modules and the coordination of these components while fulfilling tasks such as stable motion planning,  decision making, obstacle avoidance, and control (test).​</li>
<li>
<a href="http://toyota.csail.mit.edu/">Toyota-CSAIL Research Center at MIT</a> - Aimed at furthering the development of autonomous vehicle technologies, with the goal of reducing traffic casualties and potentially even developing a vehicle incapable of getting into an accident.</li>
<li>
<a href="http://vision.princeton.edu/research.html">Princeton Vision &amp; Robotics</a> - Autonomous Driving and StreetView.</li>
</ol>

<h2>
<a id="datasets" class="anchor" href="#datasets" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Datasets</h2>

<ol>
<li>
<a href="https://github.com/udacity/self-driving-car/tree/master/datasets">Udacity</a> - Udacity driving datasets released for <a href="https://www.udacity.com/self-driving-car">Udacity Challenges</a>. Contains ROSBAG training data. (~80 GB).</li>
<li>
<a href="https://archive.org/details/comma-dataset">Comma.ai</a> - 7 and a quarter hours of largely highway driving. Consists of 10 videos clips of variable size recorded at 20 Hz with a camera mounted on the windshield of an Acura ILX 2016. In parallel to the videos, also recorded some measurements such as car's speed, acceleration, steering angle, GPS coordinates, gyroscope angles. These measurements are transformed into a uniform 100 Hz time base.</li>
<li>
<a href="http://robotcar-dataset.robots.ox.ac.uk/">Oxford's Robotic Car</a> - over 100 repetitions of a consistent route through Oxford, UK, captured over a period of over a year. The dataset captures many different combinations of weather, traffic and pedestrians, along with longer term changes such as construction and roadworks.</li>
<li>
<a href="http://www.cvlibs.net/datasets/kitti/raw_data.php">KITTI Vision Benchmark Suite</a> - 6 hours of traffic scenarios at 10-100 Hz using a variety of sensor modalities such as highresolution
color and grayscale stereo cameras, a Velodyne 3D laser scanner and a high-precision GPS/IMU inertial navigation system.</li>
<li>
<a href="http://robots.engin.umich.edu/nclt/">University of Michigan North Campus Long-Term Vision and LIDAR Dataset</a> -  consists of omnidirectional imagery, 3D lidar, planar lidar, GPS, and proprioceptive
sensors for odometry collected using a Segway robot.</li>
<li>
<a href="http://robots.engin.umich.edu/SoftwareData/Ford">University of Michigan Ford Campus Vision and Lidar Data Set</a> - dataset collected by an autonomous ground vehicle testbed, based upon a modified Ford F-250 pickup truck. The vehicle is outfitted with a professional (Applanix POS LV) and consumer (Xsens MTI-G) Inertial Measuring Unit (IMU), a Velodyne 3D-lidar scanner, two push-broom forward looking Riegl lidars, and a Point Grey Ladybug3 omnidirectional camera system.</li>
<li>
<a href="http://cvssp.org/data/diplecs/">DIPLECS Autonomous Driving Datasets (2015)</a> - dataset was recorded by placing a HD camera in a car driving around the Surrey countryside. The dataset contains about 30 minutes of driving. The video is 1920x1080 in colour, encoded using H.264 codec. Steering is estimated by tracking markers on the steering wheel. The car's speed is estimated from OCR the car's speedometer (but the accuracy of the method is not guaranteed).</li>
<li>
<a href="http://www.mrt.kit.edu/z/publ/download/velodyneslam/dataset.html">Velodyne SLAM Dataset from Karlsruhe Institute of Technology</a> -  two challenging datasets recorded with the Velodyne HDL64E-S2 scanner in the city of Karlsruhe, Germany.</li>
<li>
<a href="http://synthia-dataset.net/">SYNTHetic collection of Imagery and Annotations (SYNTHIA)</a> - consists of a collection of photo-realistic frames rendered from a virtual city and comes with precise pixel-level semantic annotations for 13 classes: misc, sky, building, road, sidewalk, fence, vegetation, pole, car, sign, pedestrian, cyclist, lanemarking.</li>
<li>
<a href="https://www.cityscapes-dataset.com/">Cityscape Dataset</a> - focuses on semantic understanding of urban street scenes.  large-scale dataset that contains a diverse set of stereo video sequences recorded in street scenes from 50 different cities, with high quality pixel-level annotations of 5 000 frames in addition to a larger set of 20 000 weakly annotated frames. The dataset is thus an order of magnitude larger than similar previous attempts. Details on annotated classes and examples of our annotations are available.</li>
<li>
<a href="http://aplicaciones.cimat.mx/Personal/jbhayet/ccsad-dataset">CSSAD Dataset</a> - Several real-world stereo datasets exist for the development and testing of algorithms in the fields of perception and navigation of autonomous vehicles. However, none of them was recorded in developing countries and therefore they lack the particular characteristics that can be found in their streets and roads, like abundant potholes, speed bumpers and peculiar flows of pedestrians. This stereo dataset was recorded from a moving vehicle and contains high resolution stereo images which are complemented with orientation and acceleration data obtained from an IMU, GPS data, and data from the car computer.</li>
<li>
<a href="http://www.6d-vision.com/scene-labeling">Daimler Urban Segmetation Dataset</a> - consists of video sequences recorded in urban traffic. The dataset consists of 5000 rectified stereo image pairs with a resolution of 1024x440. 500 frames (every 10th frame of the sequence) come with pixel-level semantic class annotations into 5 classes: ground, building, vehicle, pedestrian, sky. Dense disparity maps are provided as a reference, however these are not manually annotated but computed using semi-global matching (sgm).</li>
<li>
<a href="http://data.selfracingcars.com/">Self Racing Cars - XSens/Fairchild Dataset</a> - The files include measurements from the Fairchild FIS1100 6 Degree of Freedom (DoF) IMU, the Fairchild FMT-1030 AHRS, the Xsens MTi-3 AHRS, and the Xsens MTi-G-710 GNSS/INS. The files from the event can all be read in the MT Manager software, available as part of the MT Software Suite, available here.</li>
</ol>

<h2>
<a id="open-source-software" class="anchor" href="#open-source-software" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Open Source Software</h2>

<ol>
<li>
<a href="https://github.com/CPFL/Autoware">Autoware</a> - Integrated open-source software for urban autonomous driving.</li>
<li>
<a href="https://github.com/commaai/openpilot">Comma.ai Openpilot</a> - an open source driving agent.</li>
<li>
<a href="https://sourceforge.net/projects/stanforddriving/">Stanford Driving Software</a> - Software Infrastructure for Stanford's Autonomous Vehicles.</li>
<li>
<a href="https://github.com/OSSDC/self-driving-car-1">GTA Robotics SDC Environment</a> - development environment ready for Udacity Self Driving Car (SDC) Challenges.</li>
<li>
<a href="http://oscc.io/">The OSCC Project</a> - A by-wire control kit for autonomous vehicle development.</li>
</ol>

<h2>
<a id="hardware" class="anchor" href="#hardware" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Hardware</h2>

<h2>
<a id="companies" class="anchor" href="#companies" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Companies</h2>

<ol>
<li><a href="https://www.cbinsights.com/blog/autonomous-driverless-vehicles-corporations-list/">33 Corporations Working On Autonomous Vehicles</a></li>
</ol>

<h2>
<a id="blogs" class="anchor" href="#blogs" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Blogs</h2>

<ol>
<li><a href="https://handong1587.github.io/deep_learning/2015/10/09/dl-and-autonomous-driving.html">Deep Learning and Autonomous Driving</a></li>
</ol>

<h2>
<a id="laws-and-legislation" class="anchor" href="#laws-and-legislation" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Laws and Legislation</h2>

<p>United States</p>

<ol>
<li><a href="https://www.dmv.ca.gov/portal/dmv/detail/vr/autonomous/testing">California Regulatory Notice</a></li>
<li><a href="http://fortune.com/2016/12/09/michigan-self-driving-cars/">Michigan Just Passed the Most Permissive Self-Driving Car Laws in the Country</a></li>
<li><a href="https://www.dmv.ca.gov/portal/dmv/detail/vr/autonomous/autonomousveh_ol316">Car accidents involving a SDC in California</a></li>
<li><a href="http://www.theinquirer.net/inquirer/news/2479432/nvidia-starts-testing-its-self-driving-cars-on-public-roads">Nvidia starts testing its self-driving cars on public roads</a></li>
</ol>
      </section>
    </div>

    
  </body>
</html>
